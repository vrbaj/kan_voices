{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T05:39:48.070665Z",
     "start_time": "2024-06-12T05:39:48.055751Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import librosa"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyze trimmed wavs",
   "id": "7bbebb115506289c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T05:39:48.153483Z",
     "start_time": "2024-06-12T05:39:48.075651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trimmed_files = Path(\".\").joinpath(\"trimmed_files\")\n",
    "print(f\"trimmed files ... {len(list(trimmed_files.rglob(\"*.wav\")))}\")\n",
    "original_files = Path(\".\").joinpath(\"datasets\")\n",
    "print(f\"original files ... {len(list(original_files.glob(\"**/*/*.wav\")))}\")"
   ],
   "id": "7e2655b04df56b5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimmed files ... 2041\n",
      "original files ... 2041\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T05:39:51.647488Z",
     "start_time": "2024-06-12T05:39:48.158464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "durations = []\n",
    "for audio_file in trimmed_files.glob(\"**/*/*.wav\"):\n",
    "    audio_data, sampling_rate = librosa.load(audio_file, sr=None)\n",
    "    duration = librosa.get_duration(y=audio_data, sr=sampling_rate)\n",
    "    durations.append(duration)\n",
    "print(f\"Shortest trimmed audio file: {min(durations)}\")\n",
    "\n",
    "durations = []\n",
    "for audio_file in original_files.glob(\"**/*/*.wav\"):\n",
    "    audio_data, sampling_rate = librosa.load(audio_file, sr=None)\n",
    "    duration = librosa.get_duration(y=audio_data, sr=sampling_rate)\n",
    "    durations.append(duration)\n",
    "print(f\"Shortest original audio file: {min(durations)}\")\n"
   ],
   "id": "d03c1bab87eb2a2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest trimmed audio file: 0.273\n",
      "Shortest original audio file: 0.3933\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Get time difference between trimmed and original",
   "id": "588b992269687634"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T05:39:54.180309Z",
     "start_time": "2024-06-12T05:39:51.651509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trimmed_files = sorted(trimmed_files.glob(\"**/*/*.wav\"))\n",
    "original_files = sorted(original_files.glob(\"**/*/*.wav\"))\n",
    "differences = []\n",
    "for trimmed, original in zip(trimmed_files, original_files):\n",
    "    audio_data_trimmed, sampling_rate = librosa.load(trimmed, sr=None)\n",
    "    trimmed_len = librosa.get_duration(y=audio_data_trimmed, sr=sampling_rate)\n",
    "    \n",
    "    audio_data_original, sampling_rate = librosa.load(original, sr=None)\n",
    "    original_len = librosa.get_duration(y=audio_data_original, sr=sampling_rate)\n",
    "    differences.append(original_len-trimmed_len)\n",
    "\n",
    "print(f\"Max diff: {max(differences)}, min diff: {min(differences)}\")"
   ],
   "id": "ae522b4ca007007c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 0.21183999999999997, min diff: 0.0\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T01:57:30.876364Z",
     "start_time": "2024-06-24T01:57:30.847774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_statistics(file_path):\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Assuming the second column is at index 1\n",
    "    second_column = data.iloc[:, 1]\n",
    "    \n",
    "    # Calculate the average (mean)\n",
    "    average = np.mean(second_column)\n",
    "    \n",
    "    # Calculate the standard deviation\n",
    "    std_deviation = np.std(second_column)\n",
    "    \n",
    "    return average, std_deviation\n",
    "\n",
    "# Example usage\n",
    "file_path = 'results/290511d6-4bec-4915-b90b-10d0ab5cb884/results.csv'  # Replace with your CSV file path\n",
    "average, std_deviation = compute_statistics(file_path)\n",
    "print(f\"Average: {average}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")\n"
   ],
   "id": "27a315a41de7680",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 0.8444983362019506\n",
      "Standard Deviation: 0.004936863129041041\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check librosa load eefect on results\n",
   "id": "a5d825c46a3cbcc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:14:22.217157Z",
     "start_time": "2024-07-01T03:14:21.845123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_stats(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Exclude the first column\n",
    "    df_excluded = df.iloc[:, 1:]\n",
    "    \n",
    "    # Compute mean and standard deviation for each column\n",
    "    means = df_excluded.mean()\n",
    "    std_devs = df_excluded.std()\n",
    "    \n",
    "    results = {}\n",
    "    # Print the results\n",
    "    for col in df_excluded.columns:\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\"Mean: {means[col]}\")\n",
    "        print(f\"Standard Deviation: {std_devs[col]}\")\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "file_path = 'results/8c7956e8-6725-48fc-95a8-1fd5a7753ee6/results.csv'  # Replace with your CSV file path\n",
    "compute_stats(file_path)\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Get the second column (index 1)\n",
    "second_column = data.iloc[:, 1]\n",
    "\n",
    "# Get the indices of the 10 highest values from the second column\n",
    "top_10_indices = second_column.nlargest(10).index\n",
    "\n",
    "# Get the entire rows for the 10 highest values\n",
    "top_10_rows = data.loc[top_10_indices]\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "# Print the 10 highest value rows\n",
    "print(\"\\nRows with the 10 highest values from the second column:\\n\", top_10_rows[\"params\"])"
   ],
   "id": "d18637065d6634d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: mean_test_accuracy\n",
      "Mean: 0.832396528973035\n",
      "Standard Deviation: 0.004756109074516514\n",
      "\n",
      "Column: mean_test_recall\n",
      "Mean: 0.819455737704918\n",
      "Standard Deviation: 0.005852096080797522\n",
      "\n",
      "Column: mean_test_specificity\n",
      "Mean: 0.8669211462450591\n",
      "Standard Deviation: 0.008891936101145092\n",
      "\n",
      "\n",
      "Rows with the 10 highest values from the second column:\n",
      " 64     {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "898    {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "109    {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "333    {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "691    {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "104    {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "709    {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "953    {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "303    {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "642    {'classifier__C': 2600, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'poly'}\n",
      "Name: params, dtype: object\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6c40604a76c32bb5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
